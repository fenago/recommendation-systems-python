{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining\n",
    "\n",
    "## Similarity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to compute Euclidean Distance. \n",
    "def euclidean(v1, v2):\n",
    "    \n",
    "    #Convert 1-D Python lists to numpy vectors\n",
    "    v1 = np.array(v1)\n",
    "    v2 = np.array(v2)\n",
    "    \n",
    "    #Compute vector which is the element wise square of the difference\n",
    "    diff = np.power(np.array(v1)- np.array(v2), 2)\n",
    "    \n",
    "    #Perform summation of the elements of the above vector\n",
    "    sigma_val = np.sum(diff)\n",
    "    \n",
    "    #Compute square root and return final Euclidean score\n",
    "    euclid_score = np.sqrt(sigma_val)\n",
    "    \n",
    "    return euclid_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define 3 users with ratings for 5 movies\n",
    "u1 = [5,1,2,4,5]\n",
    "u2 = [1,5,4,2,1]\n",
    "u3 = [5,2,2,4,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean(u1, u2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean(u1, u3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice = [1,1,3,2,4]\n",
    "bob = [2,2,4,3,5]\n",
    "\n",
    "euclidean(alice, bob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eve = [5,5,3,4,2]\n",
    "\n",
    "euclidean(eve, alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "pearsonr(alice, bob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(alice, eve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the function that enables us to plot clusters\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "#Get points such that they form 3 visually separable clusters\n",
    "X, y = make_blobs(n_samples=300, centers=3,\n",
    "                       cluster_std=0.50, random_state=0)\n",
    "\n",
    "\n",
    "#Plot the points on a scatterplot\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the K-Means Class\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#Initializr the K-Means object. Set number of clusters to 3, \n",
    "#centroid initilalization as 'random' and maximum iterations to 10\n",
    "kmeans = KMeans(n_clusters=3, init='random', max_iter=10)\n",
    "\n",
    "#Compute the K-Means clustering \n",
    "kmeans.fit(X)\n",
    "\n",
    "#Predict the classes for every point\n",
    "y_pred = kmeans.predict(X)\n",
    "\n",
    "#Plot the data points again but with different colors for different classes\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=50)\n",
    "\n",
    "#Get the list of the final centroids\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "#Plot the centroids onto the same scatterplot.\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=100, marker='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List that will hold the sum of square values for different cluster sizes\n",
    "ss = []\n",
    "\n",
    "#We will compute SS for cluster sizes between 1 and 8.\n",
    "for i in range(1,9):\n",
    "    \n",
    "    #Initlialize the KMeans object and call the fit method to compute clusters \n",
    "    kmeans = KMeans(n_clusters=i, random_state=0, max_iter=10, init='random').fit(X)\n",
    "    \n",
    "    #Append the value of SS for a particular iteration into the ss list\n",
    "    ss.append(kmeans.inertia_)\n",
    "\n",
    "#Plot the Elbow Plot of SS v/s K\n",
    "sns.pointplot(x=[j for j in range(1,9)], y=ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the half moon function from scikit-learn\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "#Get access to points using the make_moons function\n",
    "X_m, y_m = make_moons(200, noise=.05, random_state=0)\n",
    "\n",
    "#Plot the two half moon clusters\n",
    "plt.scatter(X_m[:, 0], X_m[:, 1], s=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize K-Means Object with K=2 (for two half moons) and fit it to our data\n",
    "kmm = KMeans(n_clusters=2, init='random', max_iter=10)\n",
    "kmm.fit(X_m)\n",
    "\n",
    "#Predict the classes for the data points\n",
    "y_m_pred = kmm.predict(X_m)\n",
    "\n",
    "#Plot the colored clusters as identified by K-Means\n",
    "plt.scatter(X_m[:, 0], X_m[:, 1], c=y_m_pred, s=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Spectral Clustering from scikit-learn\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "#Define the Spectral Clustering Model\n",
    "model = SpectralClustering(n_clusters=2, affinity='nearest_neighbors')\n",
    "\n",
    "#Fit and predict the labels\n",
    "y_m_sc = model.fit_predict(X_m)\n",
    "\n",
    "#Plot the colored clusters as identified by Spectral Clustering\n",
    "plt.scatter(X_m[:, 0], X_m[:, 1], c=y_m_sc, s=50);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset into Pandas DataFrame\n",
    "iris = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", \n",
    "                 names=['sepal_length','sepal_width','petal_length','petal_width','class'])\n",
    "\n",
    "#Display the head of the dataframe\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Standard Scaler from scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Separate the features and the class\n",
    "X = iris.drop('class', axis=1)\n",
    "y = iris['class']\n",
    "\n",
    "# Scale the features of X\n",
    "X = pd.DataFrame(StandardScaler().fit_transform(X), \n",
    "                 columns = ['sepal_length','sepal_width','petal_length','petal_width'])\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Intialize a PCA object to transform into the 2D Space.\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "#Apply PCA\n",
    "pca_iris = pca.fit_transform(X)\n",
    "pca_iris = pd.DataFrame(data = pca_iris, columns = ['PC1', 'PC2'])\n",
    "\n",
    "pca_iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate the class variable\n",
    "pca_iris = pd.concat([pca_iris, y], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the scatterplot\n",
    "sns.lmplot(x='PC1', y='PC2', data=pca_iris, hue='class', fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "#Define the LDA Object to have two components\n",
    "lda = LinearDiscriminantAnalysis(n_components = 2)\n",
    "\n",
    "#Apply LDA\n",
    "lda_iris = lda.fit_transform(X, y)\n",
    "lda_iris = pd.DataFrame(data = lda_iris, columns = ['C1', 'C2'])\n",
    "\n",
    "#Concatenate the class variable\n",
    "lda_iris = pd.concat([lda_iris, y], axis = 1)\n",
    "\n",
    "#Display the scatterplot\n",
    "sns.lmplot(x='C1', y='C2', data=lda_iris, hue='class', fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "\n",
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the dataset into the feature dataframe and the target class series.\n",
    "X, y = iris.drop('class', axis=1), iris['class']\n",
    "\n",
    "#Split the data into training and test datasets. \n",
    "#We will train on 75% of the data and assess our performance on 25% of the data\n",
    "\n",
    "#Import the splitting funnction\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "#Import the Gradient Boosting Classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#Apply Gradient Boosting to the training data\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "#Compute the accuracy on the test set\n",
    "gbc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display a bar plot of feature importances\n",
    "sns.barplot(x= ['sepal_length','sepal_width','petal_length','petal_width'], y=gbc.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
